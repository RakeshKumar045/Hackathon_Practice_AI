{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import unicodedata \n",
    "import re\n",
    "import time\n",
    "import io\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 5s 2us/step\n"
     ]
    }
   ],
   "source": [
    "#Download data\n",
    "\n",
    "path_zip = keras.utils.get_file('spa-eng.zip' , \n",
    "                                origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "                                extract = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RKumarX0105498\\.keras\\datasets/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = os.path.dirname(path_zip)+\"/spa-eng/spa.txt\"\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def convert_unicode_file(s):\n",
    "#     return \"\".join(c for c in unicodedata.normalize('NED', s) \n",
    "#                    if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "    return \"\".join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != \"Mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = convert_unicode_file(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stack w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "#     overflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        \n",
    "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "     # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "\n",
    "def create_dataset(path, no_example):\n",
    "    lines = io.open(path, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    \n",
    "    word_pair = [    [  preprocess_sentence(w) for w in l.split('\\t')  ]       for l in lines[:no_example]       ]\n",
    "    \n",
    "    return zip(*word_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tensor(tensor):\n",
    "    return max(len(m)  for m in tensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(lang):\n",
    "    long_tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
    "    long_tokenizer.fit_on_texts(lang)\n",
    "    tensor = long_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
    "    return tensor, long_tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, no_example = None):\n",
    "    #create clean input, output pair\n",
    "    tar_lang, inp_lang = create_dataset(path, no_example)\n",
    "    inp_tensor, inp_long_tokenizer = tokenizer(inp_lang)\n",
    "    tar_tensor, tar_long_tokenizer = tokenizer(tar_lang)\n",
    "    \n",
    "    return inp_tensor, tar_tensor, inp_long_tokenizer, tar_long_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1  135    3 ...    0    0    0]\n",
      " [   1  293    3 ...    0    0    0]\n",
      " [   1  595    3 ...    0    0    0]\n",
      " ...\n",
      " [   1   18 9413 ...    0    0    0]\n",
      " [   1   63 2490 ...    0    0    0]\n",
      " [   1   23 2175 ...    0    0    0]]\n",
      "[[ 1 36  3 ...  0  0  0]\n",
      " [ 1 36  3 ...  0  0  0]\n",
      " [ 1 36  3 ...  0  0  0]\n",
      " ...\n",
      " [ 1 16 38 ...  0  0  0]\n",
      " [ 1 16 38 ...  0  0  0]\n",
      " [ 1 16 38 ...  0  0  0]]\n",
      "<keras_preprocessing.text.Tokenizer object at 0x00000221B8B25208>\n",
      "<keras_preprocessing.text.Tokenizer object at 0x00000221BF129E10>\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "\n",
    "no_example = 30000\n",
    "inp_tensor, tar_tensor, inp_long_tokenizer, tar_long_tokenizer = load_dataset(path_to_file , no_example)\n",
    "\n",
    "#calculate maximum length of target tensor\n",
    "max_len_tar, max_len_inp = max_tensor(tar_tensor) , max_tensor(inp_tensor)\n",
    "\n",
    "\n",
    "print(inp_tensor)\n",
    "print(tar_tensor)\n",
    "print(inp_long_tokenizer)\n",
    "print(tar_long_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(max_len_tar)\n",
    "print(max_len_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "# input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(inp_tensor, tar_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(inp_tensor, tar_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "# Show length\n",
    "# print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "print(len(x_train), len(x_test), len(y_train), len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
